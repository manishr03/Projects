---
title: "STA141A Final Project"
author: "Manish Rathor and Victor Bulyga"
date: "9/10/2023"
output: pdf_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(knitr)
library(dplyr)
library(xts)
library(quantmod)
library(forecast)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE, warning = FALSE, message = FALSE)
opts_knit$set(root.dir = "/Users/manishrathor/Documents/School /Davis /2023-24/SS2 2023/STA141A/Final Project/archive")
```

# Abstract

In this project, we trained an ARIMA model to predict future Google closing stock prices using data from 2018-2023. We first imported the dataset and performed an exploratory analysis to understand its contents. Once we developed an understanding of the data's components, we cleaned and transformed the data so it was suitable for visualization. After that, we created relevant visualizations to analyze trends and the overall shape of the data. The visualizations, along with our prior knowledge of stock market and time-series data, confirmed that we needed to train an ARIMA model for accurate forecasting. Before training the model, we had to transform the data again, because the relevant time-series functions in R cannot be applied to data frames. After performing the necessary transformations, we created new visualizations to further understand underlying trends within the data. Then, we were able to train the model. We split the original dataset into a training and test set, trained the model on the training set, and created a forecast with the model. We then compared the test data to the forecast through relevant visualizations, which gave us a rough idea of the model's accuracy. After that, we tested the model's performance by first checking to ensure that it satisfied relevant assumptions, and then evaluating its accuracy through its RMSE. Once we were confident that our model was accurate, we decided to use it in a simulation. Through this simulation, we determined that the model would generate profit, although due to Google's abnormally high stock value, it did not generate as much profit as naively buying shares and never selling. Finally, we included a brief discussion regarding the applications and importance of this type of analysis. 

# Introduction 

In the Big Data Era, harnessing data and statistical models has allowed businesses to create accurate predictions for all sorts of problems. Using these models has increased efficiency and allowed firms to guide their decision-making processes with concrete insights. 

One instance of this is seen in the financial industry. Investment firms, which generate profits through the buying and selling of financial commodities, play a risky game. They hedge their bets on the success or failure of the commodities they purchase, and can experience financial failure if their gambles don't pay off. To minimize the risk of their bets, these companies rely on data-driven predictions. They train machine learning models to predict which commodities will succeed and fail, allowing them to buy and sell with minimal risk. 

In our project, we will train a model that accurately predicts future stock values. We will specifically analyze Google stock data ranging from July 30, 2018 to July 28, 2023. The data we will use contains the daily opening, high, low, closing, and adjusted closing prices, as well as the daily volume of the stock. 

* Opening Price: The price of a stock when the market opens 
* High Price: The highest price of a stock in a given period
* Low Price: The lowest price of a stock in a given period
* Closing Price: The price of a stock when the market closes 
* Adjusted Closing Price: The closing price after adjustments for splits and dividend distributions
* Volume: The number of shares traded in a given period

To simplify this analysis, we will solely focus on the daily closing price of the stock. This is an industry standard, as most firms either use closing price of adjusted closing price data to train their models. 

Our first step will be to handle and pre-process the data. To ensure that we can implement the necessary functions and accurately train the model, we will convert our data to the right types and remove any corrupt/incomplete data. 

After handling and pre-processing, we will visualize the data. Creating visualizations will help us decide which methodology to use while selecting and training our model. 

Once we've visualized the model, we will select the appropriate statistical model and train it. 

Finally, after selecting, training, and implementing the model, we will analyze its performance and discuss the implications of our findings. 

# Data Wrangling/Preparation

We start by importing the data. As seen below, the data is imported from a .csv file and is stored as a data.frame. As stated above, the data contains the opening, high, low, closing, and adjusted closing stock prices for Google. We can also see that the observations begin on July 30, 2018. 
```{r}
# Loading in the dataset
google.df <- read.csv("GOOG.csv")
kable(head(google.df), "pipe") 
class(google.df)
```

We use this function to find the final observation of the data, which occurs on July 28, 2023.  
```{r}
# Finding the final date in the data
kable(tail(google.df, n=1), "pipe")
```

Our first step in pre-processing the data is converting the Date values to date type. As you can see above, they are currently stored as characters, which prevents us from visualizing and modeling the data. The function below converts the values from character to date. 
```{r}
# Converting the data to date type 
google.df$Date <- as.Date(google.df$Date, format = "%m/%d/%y")
kable(head(google.df), "pipe")
```
Now that we have converted the Date to date type, we will convert the Volume to type double. While it is already stored as a numeric, meaning that it can be used in our statistical analysis, we will convert it to double to maintain consistency with the rest of the data. 
```{r}
# Converting the Volume to numeric type 
google.df <- google.df %>%
  mutate(Volume = as.numeric(Volume)) %>%
  na.omit(.) %>%
  subset(select = c(Date, Open, High, Low, Close, Volume))
kable(head(google.df), "pipe")
```

# Data Visualization

Now that we have converted the Date to date type, and ensured that the rest of the values are doubles, we can visualize our data. As we said before, we will be focusing on the closing price, so we will use a line graph to visualize the closing price across all five years. To do this, we will use the ggplot2 package, which allows us to create aesthetic visualizations with clean, concise code. 
```{r}
# Visualizing closing price data 
google.df %>%
  ggplot(aes(x = Date, y = Close)) + 
  geom_line()
```
Based on the visualization, we can see that while the data follows a general trend, there are fluctuations throughout. We can conclude from this visualization, and from our understanding of stock market data that a time-series analysis will be required. Time-series models are able to account for the fluctuations (known as seasonality) and create accurate predictions. 

# A Brief Introduction to Time-Series Analysis

Most of the data we are used to is made up of observations that contain the same features (predictors) and target (output). When developing regression models, we train the models to predict future target values using the features. For example, if we are given a dataset that contains the age of 20 women, and their respective muscle mass, we can train a linear regression model to predict muscle mass for women at different ages. The model will do this by analyzing the relationship between age (feature) and muscle mass (target). It will then generate a mathematical formula that can accept a hypothetical age value and output a hypothetical muscle mass. 

Time-Series data differs in the sense that there are no features. Instead, the target data is collected repeatedly over a certain period of time. Stock market data is an example of this. The price of a stock is recorded on a daily basis and is stored in a table. To model and predict this data accurately, we have to take a different approach. Rather than using features to predict the target, we have to use previous target values to predict future target values. This will be demonstrated below. 

# More Data Wrangling/Preparation

In R, the relevant functions to implement a time-series analysis can be found in the quantmod, xts, and forecast libraries However, for the functions within the libraries to work, we need to transform the data into a new type. 

Because we have already tidied our data previously, the only required step is to transform the data from class data.frame to class xts. Converting the data to this format will allow the functions from the previously desribed libraries to work.
```{r}
# Converting dataframe to ts class
google.ts <- as.xts(google.df)
kable(head(google.ts), "pipe")
class(google.ts)
```
As we can see, the data is now in class xts. 

# More Data Visualization

Now that we have transformed the data, it is a good idea to visualize it. Rather than using the ggplot2 package, we will use a function from the quantmod package, which is designed to visualize stock market data. 
```{r}
# Visualizing the data with a new chart
chartSeries(google.ts, name = "Google Price 2018-2023")
```
As we can see, the chart not only contains the line graph of the stock data, but it also contains a visualization of the volume traded. Much of this is unnecessary for our analysis, but it is a good way to visualize a stock's overall performance. We can also see that the chart looks the same as the one produced using the ggplot2 package, confirming that none of the values in the data have been changed.

There is one more important visualization we would like to look at: a decomposition graph. This graph will visualize the different trends within the data more effectively than our current visualizations. However to do this, we need to simplify the data. 

We will simplify the data by creating a new xts that only contains the monthly closing price for the stock. By transforming the data so that there is only one closing price per month, we can effectively generate a decomposition graph. 
```{r}
# Filtering the ts object to only include monthly closing prices 
google.ts.close <- Cl(to.monthly(google.ts))
kable(head(google.ts.close), "pipe")
class(google.ts.close)
```
As we can see, the new object contains a single closing price for each month, and remains in the class xts. 

To create the decomposition graph, we will apply the decompose() function to the new xts object we've created. 
```{r}
# Decomposing the data to visualize relevant trends 
dc <- decompose(as.ts(google.ts.close))
plot(dc)
```
As we can see, the decomposition graph gives us four different charts: Observed, Trend, Seasonal, Random. Time is written a 1-6, but represents 2018-2023. 

* Observed: This is the original plot of the data. The observed data is shown below. 
```{r}
dc$x
```
* Trend: This visualizes long term movements in the data. We can see that it generally reflects the shape of the Observed graph, but does not contain the minor fluctuations. The trend data is shown below. 
```{r}
dc$trend
```
* Seasonal: This visualizes repetitive fluctuation in the data. As we can see, the closing price is highest at the beginning of the year, and then steeply drops to its lowest right after. This indicates to firms that they should buy Google stock a month or two after the year begins, and sell right at the beginning of the next year. The seasonal data is shown below. 
```{r}
dc$seasonal
```
* Random: This visualizes random fluctuations in the data not accounted for by the trend and/or seasonality. This is also known as white noise. Because much of the data was collected during the COVID-19 pandemic, there is quite a bit of white noise. It is important to note that larger amounts of white noise will lead to less accurate models, something to consider during the analysis. The random data is shown below. 
```{r}
dc$random
```

# Modeling/Forecasting

Now that we have appropriately wrangled/pre-processed our data, and have also visualized it effectively, it is time to select and train our model. 

The specific model we will use is the Auto-Regressive Integrated Moving Average (ARIMA) model. The ARIMA model stems from the idea that current target values depend on previous target values, as well as previous changes and errors. This model is able to account for this through three parameters: P, D, and Q. P represents the number of autoregressive terms in the model, D represents the number of times the data is differenced, and Q represents the number of moving average terms in the model. 

The ARIMA model is advantageous for a few reasons. Firstly, it can handle a wide range of univariate time series data. Also, it can account for trends, volatility, and seasonality. The ARIMA model is also easy to use, as it doesn't require many parameters or assumptions. Finally, because it is developed using statistical methods/theory, it generally produces accurate forecasts. 

It is also important to note that the ARIMA model is one of the most widely used models in financial analysis (specifically for predicting stock performance). Based on the advantages described above, the characteristics of our data (univariate with seasonality and trend), and the fact that investment firms use the model often, we believe that it is an appropriate fit.

To train the model, we need to create a training and test dataset. We will do this by splitting our current data into two new datasets. Specifically, we will select the final 100 observations to be the test data, and keep the remaining data as the training data. 
```{r}
# Splitting the dataset for training and test sets 
n <- 100
train <- head(Cl(google.ts), length(Cl(google.ts)) -n)
test <- tail(Cl(google.ts), n)
```

Now that we have created our training and test data, we can train the ARIMA model. The auto.arima() function creates the model, while the forecast() function will forecast future observations. We set h = n so that the forecast generates 100 future observations.

The auto.arima() function automatically selects the optimal model by using a variation of the Hyndman-Khandakar algorithm. The algorithm uses unit root tests and minimizes the AICc and MLE to generate the best parameters. This means that we do not have to manually select parameters and run diagnostic tests. Instead, the function will do the work for us and return the best model. 

We will also train two different models. One will account for seasonality, and one won't. If both models return the same values, we can conclude that seasonality isn't a significant part of the data, and we can therefore use the non-seasonal model for our prediction. 

```{r}
# Training a non-seasonal ARIMA model  
model <- auto.arima(train, seasonal = F) 
forecast <- forecast(model, h = n)
model
```

```{r}
# Training the seasonal ARIMA model
model.s <- auto.arima(train)
forecast.s <- forecast(model.s, h = n)
model.s
```

As we can see, the seasonal and non-seasonal models generate the same values. This means that seasonality is not significant, and we can use the non-seasonal model for our prediction. 

We can also visualize the models. Because both models are the same, their visualizations will also be the same. However, we will generate charts for both to maintain consistency. 

```{r}
# Visualizing the non-seasonal model
autoplot(forecast) + 
  autolayer(ts(test, start = length(train)), series = "Test Data")
```

```{r}
# Visualizing the seasonal model
autoplot(forecast.s) + 
  autolayer(ts(test, start = length(train)), series = "Test Data")
```

Based on the visualization, we can see that the model offers a good, but not great, forecast of the data. We can draw this conclusion from the confidence interval visualization on the right side of the graph. In this visualization, the thin dark blue line represents a 100% confidence interval, the dark blue arc represents a 95% confidence interval, and the light blue arc represents an 80% confidence interval. As we can see, the test data falls within the light blue arc, meaning that it falls within the 80% confidence interval. Therefore, the forecast is able to somewhat capture the test data values, but there is room for improvment. 

# Results 

Now that we have trained our model and forecasted our data, we can evaluate the performance. 

The first step in this process is to check if the residuals meet the residual assumptions. The assumptions are listed below 

* Residuals must be normally distributed 
* Residuals must have constant variance 
* Residuals must not be autocorrelated

```{r}
# Checking the residuals of the model
checkresiduals(forecast)
```
As we can see from the histogram, the residuals are normally distributed. Meanwhile, the residual plot shows that the variance is constant. Finally, to check for autocorrelation, we use the ACF plot and the Ljung-Box Test. With the ACF plot, if the lines extend beyond the upper or lower bound, there may be autocorrelation. With the Ljung-Box test, if the p-value is less than 0.05, there may be autocorrelation. However, in the case that the ACF plot and the Ljung-Box test produce different results, we will use the result from the Ljung-Box test.As we can see, while the ACF plot suggests autocorrelation, the p-value from the Ljung-Box test suggests the opposite. Because the two tests produce different results, we will use the Ljung-Box test result, and conclude that there is no autocorrelation. 

Finally, we can check the accuracy of our model. We will use the accuracy() function to do this. However, the function generates a variety of statistics, so we will choose to focus on the Root Mean Squared Error (RMSE). The RMSE is the standard deviation of the residual, essentially measuring the variance of the residuals. Intuitively, we want a lower RMSE value.
```{r}
# Checking the accuracy of the model
accuracy(forecast)
```
The guidelines for determining a "good" RMSE are dependent on the data being forecasted. Datasets with larger numbers should have larger RMSE values, while datasets with smaller numbers should have smaller RMSE values. Based on the numbers in the stock market data, it is reasonable to conclude that an RMSE value of 1.863 is very good. This means that the model is very accurate. 

Another way to check a model's accuracy is through the normalized RMSE value. The formula for a normalized RMSE is RMSE/(max value - min value). For a normalized RMSE to be considered "good", it should fall between 0 and 1. 
```{r}
# Normalized RMSE calculations
RMSE <- 1.862604
maxClose <- max(google.df$Close)
minClose <- min(google.df$Close)
nRMSE <- RMSE / (maxClose - minClose)
nRMSE
```
As we can see, our RMSE is very good. A value this close to zero means that the model can be considered accurate.

# Simulation 

Now that we have determined that our model is accurate, we want to test it in a real-world scenario. To demonstrate this, we will create a simulation in which a trader will use the model to guide their buying/selling process. The trader will begin trading on July 30, 2018, with $1,000 and zero shares. Every thirty days, the trader will buy or sell based on the model's predictions and certain parameters we set. This process will repeat until we reach the present date. 

```{r}
#Given the history of a stock,
# create a prediction of that stock's possible values
stockForecast <- function(stock.history) {
  train <- head(stock.history, nrow(stock.history) -n)

  # Training a seasonal ARIMA model  
  model <- auto.arima(train)
  forecast <- forecast(model, h = n)
  
  return(forecast)
}

#Store information about simulated stock trader
trader.info <- data.frame(Liquid.Funds = 1000, Shares.Owned = 0)

tradeDecision <- function(stock.history, trader.info, next.choice) {
  price <- as.numeric(tail(stock.history, 1))
  funds <- trader.info$Liquid.Funds
  owned <- trader.info$Shares.Owned
  can.buy <- funds %/% price
  forecast <- stockForecast(stock.history)
  
# first determine if trader is waiting to buy or waiting to sell
  if (owned == 0) {
# if waiting to buy, wait until the stock doesn't have the risk of dropping
# significantly in value by the time of the next decision.
    if (as.numeric(forecast$lower[next.choice, 2]) > 0.7 * price) {
      choice <- "buy"
    } else {
      choice <- "hold"
    }
  } else {
# if waiting to sell, hold until the stock doesn't show promise for increasing
# significantly by the time of the next decision.
    if (as.numeric(forecast$upper[next.choice, 1]) < 1.01 * price) {
      choice <- "sell"
    } else {
      choice <- "hold"
    }
  }
  
# based on the current choice, make the changes to the trader's portfolio
  trader.info <- switch (choice,
    "buy" = data.frame(Liquid.Funds = round(funds - can.buy*price, 2),
                       Shares.Owned = owned + can.buy),
    "sell" = data.frame(Liquid.Funds = round(funds + owned*price, 2),
                       Shares.Owned = 0),
    "hold" = trader.info
  )
  return(trader.info)
}

trader.info <- tradeDecision(Op(google.ts), trader.info, 30)
```

```{r}
trader.info <- data.frame(Liquid.Funds = 1000, Shares.Owned = 0)
trader.history <- trader.info
next.choice <- 30
n <- 100

#This for-loop simulates a trader making decisions each month by updating the
#trader's portfolio with the decision function.
#All of the decisions of the trader are recorded.
for (i in seq(2 * n, nrow(google.ts), next.choice)) {
  partial.history <- head(google.ts, i)
  
  trader.info <- tradeDecision(Op(partial.history), trader.info, next.choice)
  trader.history <- rbind(trader.history, trader.info[1,])
}

kable(tail(trader.history), "pipe", align ="c")
17*as.numeric(tail(Op(google.ts), 1))
```
Using our model, the trader would've had a final portfolio value of \$2,226.49. This means that they would've experienced a \$1,226.49 profit. However, if the buyer had simply bought in 2018 and never sold, they would've had a final portfolio value of \$2,261.17, resulting in a \$1,261.17 profit. Despite this, it doesn't necessarily mean that the model is bad. Instead, this situation is more reflective of the fact that Google's stock grows at an abnormally fast and consistent rate, without ever experiencing a significant drop in value. If the model is applied to most other stocks, which don't perform as well as Google, it is reasonable to infer that it would produce positive results. 

# Conclusion

As we can see from the forecasting results and the trading simulation, time-series modeling is a powerful way to successfully trade on the stock market. Applying this technique to the financial industry will allow businesses to improve their profits and minimize risk. 

Beyond this model, there are also be other ways to predict stock performance. Using company information as features may yield accurate performance predictions. Also, using an algorithmic stock selection technique to find companies that will perform well is a viable idea. 

Overall, the use of statistical modeling in the financial industry is an innovative way to improve on outdated trading methods. 